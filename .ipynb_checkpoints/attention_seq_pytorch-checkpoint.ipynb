{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention, Beamsearch seq2seq - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: [CMU spelling dictionary](http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b) <br>\n",
    "The goal of this notebook is to implement a seq2seq attention model and a regular seq2seq is implemented alongside to gain a more complete picture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this notebook contains mapping of words in English and their pronounciations as a set of phoneme word pairs like so <br>\n",
    "<small><p style=\"margin-left: 40px\">S-AE1-N-AH0-T-IY0   sanity</p></small>\n",
    "The task is to use a seq2seq model to learn this mapping so that given a set of phonemes, the model outputs the correct word. It can be seen as emulating spelling bee. Given below is an image of the results obtained from a regular seq2seq model\n",
    "<img src=\"seq2seq.png\">\n",
    "The predictions highlighted with red border shows a common problem with seq2seq- If only the context vector, i.e., the last hidden state in the encoder network is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence, extending upto the last timestep of the decoder. \n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every time step of the decoder's own outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:49.608795Z",
     "start_time": "2018-02-09T06:49:48.454325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "import time, math, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import pdb\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "Path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:49.613183Z",
     "start_time": "2018-02-09T06:49:49.609754Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:49.988009Z",
     "start_time": "2018-02-09T06:49:49.614150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', ['AH0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get each word that begins with A-Z from each line into a list \n",
    "lines = [l.strip().split(\"  \") for l in open(Path+'cmudict-0.7b', encoding='latin1') \n",
    "         if re.match('^[A-Z]', l)]\n",
    "#Split words and phonemes\n",
    "lines = [(w, ps.split()) for w, ps in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.048791Z",
     "start_time": "2018-02-09T06:49:49.988895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of all the unique phonemes from lines and adding _ to position 0 because it corresponds to padding\n",
    "#when tokenised\n",
    "phonemes = [\"_\"]+sorted(set(p for w, ps in lines for p in ps))\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.052302Z",
     "start_time": "2018-02-09T06:49:50.049704Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map phonemes to indices and letters to indices.\n",
    "p2i = dict((v, k) for k, v in enumerate(phonemes))\n",
    "letters = \"_abcdefghijklmnopqrstuvwxyz*\"\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.294275Z",
     "start_time": "2018-02-09T06:49:50.053248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start of sentence token\n",
    "SOS_token = 0\n",
    "\n",
    "maxlen = 15\n",
    "#Map words to corresponding list of phoneme indices with constraints. \n",
    "pronounce_dict = {w.lower(): [p2i[p] for p in ps] for w, ps in lines\n",
    "                    if (5<=len(w)<=maxlen) and re.match(\"^[A-Z]+$\", w)}\n",
    "len(pronounce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.307199Z",
     "start_time": "2018-02-09T06:49:50.295575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_p = max([len(v) for k,v in pronounce_dict.items()]); maxlen_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.858009Z",
     "start_time": "2018-02-09T06:49:50.308461Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words contain the number of words in the filtered dictionary\n",
    "words = np.random.permutation(list(pronounce_dict.keys()))\n",
    "n = len(words)\n",
    "\n",
    "#Initialise the input and labels array with zeros so that everywhere except \n",
    "#the position of values is padded\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "#Fill in the non zero indices\n",
    "for i, k in enumerate(words):\n",
    "    for j, p in enumerate(pronounce_dict[k]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.868393Z",
     "start_time": "2018-02-09T06:49:50.858989Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test, \n",
    "    ) = train_test_split(input_, labels_, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.872163Z",
     "start_time": "2018-02-09T06:49:50.869480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.875730Z",
     "start_time": "2018-02-09T06:49:50.873191Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=128):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.880690Z",
     "start_time": "2018-02-09T06:49:50.876733Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 240 #Hidden units\n",
    "batch_size = 128\n",
    "ln = 0.7 #Length normalization parameter for beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.910338Z",
     "start_time": "2018-02-09T06:49:50.881937Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size//2)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.grubi = nn.GRU(hidden_size//2, hidden_size//2, dropout=dropout_p, batch_first=True, num_layers=1,\n",
    "                         bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=dropout_p,\n",
    "                            num_layers=1)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=dropout_p,\n",
    "                            num_layers=1)\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input) \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x, hidden = self.grubi(x, hidden)\n",
    "        #Concatenating hidden state to get a single layer because\n",
    "        #bidirectional return a layer for each direction. \n",
    "        hidden = torch.cat(torch.chunk(hidden, 2, 0),2)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output, hidden = self.gru2(x, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #2 for bidirectional, change to 1 otherwise.\n",
    "        return Variable(torch.zeros(2, batch_size, self.hidden_size//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.925449Z",
     "start_time": "2018-02-09T06:49:50.911673Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) #Optional\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        #self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.embedding(input).unsqueeze(1)\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        #res, hidden = self.gru2(res, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Decoder with Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "#### Attention module\n",
    "<img src=\"formular.png\">\n",
    "Attn class implements the above equations <br>\n",
    "L - Number of layers,\n",
    "B - Batch size,\n",
    "H - Hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.945367Z",
     "start_time": "2018-02-09T06:49:50.926790Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(nn.init.xavier_normal(torch.rand(1, hidden_size)))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden - hidden state of previous layer [LxBxH]\n",
    "            encoder_outputs - encoder outputs for each timestep [BxTxH]\n",
    "        Output: \n",
    "            Normalized weightings for each of the encoder output [BxT]\n",
    "        '''\n",
    "        maxlen_p = encoder_outputs.size(1) \n",
    "        H = hidden.repeat(maxlen_p, 1, 1).transpose(0,1)\n",
    "        weights = self.attn(torch.cat([H, encoder_outputs], 2)) #[BxTx2H]->[BxTxH]\n",
    "        weights = self.tanh(weights) \n",
    "        weights = weights.transpose(2,1) #[BxHxT]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[Bx1xH]\n",
    "        weights = torch.bmm(v, weights) #[Bx1xH]*[BxHxT]->[Bx1xT]\n",
    "        weights = weights.squeeze(1) #softmax requires 2D tensor\n",
    "        return self.softmax(weights).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:50.978703Z",
     "start_time": "2018-02-09T06:49:50.946678Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, maxlen_p, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.maxlen_p = maxlen_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, batch_first=True, num_layers=1, dropout=dropout_p)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1, dropout=dropout_p)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden, enc_outputs):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input - initial token, decoder output or target(teacher forcing) [B] \n",
    "            hidden - hidden state of previous layer [LxBxH]\n",
    "            encoder_outputs - encoder outputs for each timestep [BxTxH]\n",
    "        Output: \n",
    "            Normalized weightings for each of the encoder output [BxT]\n",
    "        '''\n",
    "        emb = self.embedding(input).unsqueeze(1) #[Bx1xH]\n",
    "        emb = self.dropout(emb) \n",
    "        weights = self.attn(hidden, enc_outputs) #[Bx1xT]       \n",
    "        context = weights.bmm(enc_outputs) #[Bx1xT]*[BxTxH]->[Bx1xH]\n",
    "        rnn_input = torch.cat((emb, context), 2) #[Bx1xH*2]\n",
    "        \n",
    "        res, hidden = self.gru(rnn_input, hidden) \n",
    "        res, hidden = self.gru(rnn_input, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I - Input sequence length\n",
    "O - Output sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:51.023230Z",
     "start_time": "2018-02-09T06:49:50.979686Z"
    },
    "cell_style": "center",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(attention, input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, teach_force_t):\n",
    "    '''\n",
    "    Inputs:\n",
    "        attention - boolean determines if attention needs to be calculated \n",
    "        input_variable - training samples [BxI]\n",
    "        target_variable - used to calculate loss and as decoder input [BxO]\n",
    "        encoder, optimizer \n",
    "        decoder, optimizer\n",
    "        criterion - loss function\n",
    "        teach_force_t - threshold after which teacher force will be used \n",
    "    Output: \n",
    "        loss\n",
    "    '''\n",
    "    batch_size= input_variable.size()[0]\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size)).cuda()\n",
    "    \n",
    "    if (random.random() > teach_force_t):\n",
    "        for di in range(target_length):\n",
    "            if attention:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)          \n",
    "\n",
    "            targ = target_variable[:, di]\n",
    "            loss += criterion(decoder_output, targ)\n",
    "            decoder_input = targ\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            if attention:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)                    \n",
    "            \n",
    "            targ = target_variable[:, di]\n",
    "            loss += criterion(decoder_output, targ)\n",
    "            _, indices = torch.max(decoder_output, 1)\n",
    "            decoder_input = indices\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:51.055855Z",
     "start_time": "2018-02-09T06:49:51.024127Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(attention, encoder, decoder, iters, print_every=1000, plot_every=100, \n",
    "                learning_rate=0.01, teacher_force_t=0.5):\n",
    "\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "\n",
    "    for iter in tqdm_notebook(range(1, iters + 1)):\n",
    "        training_batch = get_batch(input_train, labels_train, 128)\n",
    "        input_variable = Variable(torch.LongTensor((training_batch[0].astype('int64')))).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(training_batch[1].astype('int64'))).cuda()\n",
    "        loss = train(attention, input_variable, target_variable, encoder, decoder, encoder_optimizer, \n",
    "                             decoder_optimizer, criterion, teacher_force_t)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Loss: ',print_loss_avg, end=\"\\r\", flush=True)\n",
    "            \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:49:51.060857Z",
     "start_time": "2018-02-09T06:49:51.056749Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:11:38.738724Z",
     "start_time": "2018-02-09T07:11:38.642957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_vocab_size, dim, dropout_p=0.3).cuda()\n",
    "#decoder = DecoderRNN(dim, output_vocab_size).cuda(); attn=False\n",
    "decoder = AttnDecoderRNN(dim, output_vocab_size, maxlen_p, dropout_p=0.3).cuda(); attn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:27.462945Z",
     "start_time": "2018-02-09T07:16:51.087944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf875be30f949d49140783a059d9ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.25928312454223635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbddc3550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRF6CgUxE5CX+P/HXRcOQ1pavAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbd5d0978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(attn, encoder, decoder,3000, print_every=500, learning_rate=0.0005, teacher_force_t=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "Beam search is a solution to the NP complete Viterbi algorithm. Viterbi algorithm, from a large set of combination of choices, tries to find the best possible combination by calculating the probabilities of every single combination and in our case, that is in each timestep. <br>\n",
    "Beam search removes low probability partial hypotheses in each timestep by choosing k most probable hypotheses in each timestep rather than a single most probable value. Although beam search has been proven to be quite effective, it fails to deliver substantial results with this dataset. <b>A possible explanation is that spelling a word maybe too constrained a problem especially under the observation that the model becomes 'rigid' after 5000 iterations and predicts with high probabilities for what it thinks is right reducing the chances of other hypotheses becoming viable</b>.\n",
    "<img src=\"beamsearchc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:42.786052Z",
     "start_time": "2018-02-09T07:21:42.621229Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(test_batch,attention, encoder, decoder, batch_size=128, ln=0.7):\n",
    "    totAcc = 0\n",
    "    preds_beam = []\n",
    "    for num in tqdm_notebook(range(batch_size)): \n",
    "        k=5\n",
    "        input_variable = Variable(torch.LongTensor((test_batch[0][num].astype('int64'))), volatile=True).cuda() #[1x16]\n",
    "        target_variable = Variable(torch.LongTensor(test_batch[1][num].astype('int64')), volatile=True).cuda() #[1x15]\n",
    "        input_variable = input_variable.unsqueeze(0)\n",
    "        target_variable = target_variable.unsqueeze(0)\n",
    "\n",
    "        target_length = target_variable.size()[1]\n",
    "        encoder_hidden = encoder.initHidden(1).cuda()\n",
    "\n",
    "        hypothesis = [] #List storing list of initial hypotheses\n",
    "        nhypothesis = [] #List to store updated hypotheses\n",
    "        lhypothesis = Variable(torch.LongTensor([0]*k)).cuda() #Variable storing the most recent entry in each of the k hypotheses\n",
    "        kprobs = Variable(torch.zeros(k)) #Probability of each hypothesis\n",
    "        hstates = Variable(torch.zeros(k, 1,240)) #Initial hidden states of hypotheses\n",
    "        nhstates = Variable(torch.zeros(k, 1,240)) #Changed hidden states of hypotheses\n",
    "        bestk_p = Variable(torch.zeros(k*k)) #Vector storing probability of best k for each of top k hypotheses\n",
    "        bestk_i = Variable(torch.zeros(k*k)) #Vector storing index of best k for each of top k hypotheses\n",
    "        \n",
    "        encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True).cuda()\n",
    "        if attention:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden)        \n",
    "        decoder_output = decoder_output.squeeze(0)\n",
    "        vals, indices = torch.topk(decoder_output, k, 0) #Finding the initial values of k hypothesis \n",
    "        for i in range(k):\n",
    "            hypothesis.append([indices[i].data.cpu().numpy().item()])\n",
    "            nhypothesis=list(hypothesis)\n",
    "            lhypothesis[i] = indices[i]\n",
    "            kprobs[i] = (vals[i].data)\n",
    "\n",
    "        repeated_probs = torch.zeros(k*k) #Vector to hold k probabilities repeated for operations \n",
    "        hstates = decoder_hidden.repeat(k, 1, 1) #Initially, every k share the same hidden state\n",
    "        for di in range(target_length-1):\n",
    "            for i in range(k):\n",
    "                decoder_input = lhypothesis[i]\n",
    "\n",
    "                decoder_hidden = hstates[i].unsqueeze(0)\n",
    "                if attention:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)                      \n",
    "                decoder_output = decoder_output.squeeze(0)\n",
    "                nvals, nindices = torch.topk(decoder_output, k, 0) #[1xk]\n",
    "                nhstates[i] = decoder_hidden\n",
    "                bestk_p[k*i:(k*i+k)] = nvals\n",
    "                bestk_i[k*i:(k*i+k)] = nindices\n",
    "            for i in range(k): repeated_probs[k*i:(k*i+k)]=kprobs[i].data.repeat(k)\n",
    "            summed_p = (repeated_probs.numpy() + bestk_p.cpu().data.numpy())\n",
    "        \n",
    "            vals, nindices = torch.topk(torch.from_numpy((1/target_length**ln)*(summed_p)), k, 0) \n",
    "            lhypothesis = Variable(torch.LongTensor([0]*k)).cuda() #Reinitialise required??\n",
    "            for i,x in enumerate(nindices):\n",
    "                kprobs[i] = float(summed_p[x])\n",
    "                lhypothesis[i]=bestk_i[x]\n",
    "                hstates[i]=nhstates[np.floor(x/k)]\n",
    "                nhypothesis[i]=list(hypothesis[int(np.floor(x/k))])\n",
    "                nhypothesis[i].append(int(bestk_i[x].data.numpy()))\n",
    "            hypothesis = list(nhypothesis)\n",
    "\n",
    "        #Accuracy calculated by comparing each letter of the target variable\n",
    "        #totAcc += (np.mean([(real==p) for real, p in zip(test_batch[1][0].tolist(), (hypothesis[0]))]))\n",
    "        preds_beam.append(np.array(hypothesis[0]))\n",
    "    preds_beam = np.asarray(preds_beam)\n",
    "    print ('Accuracy', np.mean([all(real==p) for real, p in zip(test_batch[1], preds_beam)])*100,'%')\n",
    "    return preds_beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:43.440994Z",
     "start_time": "2018-02-09T07:21:43.439214Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:43.958069Z",
     "start_time": "2018-02-09T07:21:43.953521Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch = get_batch(input_test, labels_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:45.240896Z",
     "start_time": "2018-02-09T07:21:45.210691Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_batch, attention, encoder, decoder, batch_size=128):  \n",
    "\n",
    "    input_variable = Variable(torch.LongTensor((test_batch[0].astype('int64'))), volatile=True).cuda()\n",
    "    target_variable = Variable(torch.LongTensor(test_batch[1].astype('int64')), volatile=True).cuda()\n",
    "\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size), volatile=True).cuda()\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    for di in range(target_length):\n",
    "        if attention:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)                \n",
    "        _, indices = torch.max(decoder_output, 1)\n",
    "        decoded_words.append(indices)\n",
    "        decoder_input = indices\n",
    "    preds = []\n",
    "    for x in decoded_words:\n",
    "        preds.append(x.cpu().data.numpy())\n",
    "    preds = np.array(preds).T\n",
    "    print ('Accuracy', np.mean([all(real==p) for real, p in zip(test_batch[1], preds)])*100,'%')\n",
    "    return (preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T07:21:46.032426Z",
     "start_time": "2018-02-09T07:21:45.970596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 46.875 %\n"
     ]
    }
   ],
   "source": [
    "preds = evaluate(test_batch, attn, encoder, decoder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T03:54:10.799918Z",
     "start_time": "2018-02-09T03:52:36.233393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8ea7dc98b34f508ea29e0b13da30c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1024), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy 53.3203125 %\n"
     ]
    }
   ],
   "source": [
    "preds_beam = beam_search(test_batch,attn, encoder, decoder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T04:33:41.752438Z",
     "start_time": "2018-02-09T04:33:41.746056Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results(preds_beam, preds, test_batch):\n",
    "    print ('  Phonemes_________________________________predictions___________beam_________________label')\n",
    "    for index in range(32, 50):\n",
    "        phoneme = '-'.join([phonemes[p] for p in test_batch[0][index]])\n",
    "        prediction = [letters[l] for l in preds[index]]\n",
    "        real = [letters[l] for l in test_batch[1][index]]\n",
    "        beamed = [letters[l] for l in preds_beam[index]]\n",
    "        print ('  ',phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(20), ''.join(beamed).strip('_').ljust(20),\n",
    "               ''.join(real).strip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T04:33:42.618539Z",
     "start_time": "2018-02-09T04:33:42.610930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Phonemes_________________________________predictions___________beam_________________label\n",
      "   L-AW1-D-IH0-N-S-L-EY0-G-ER0              laudenslager         loudenslager         loudenslager\n",
      "   P-AH0-L-IY1-S-W-UH2-M-AH0-N              polisewoman          policewoman          policewoman\n",
      "   V-EH0-K-S-EY1-SH-AH0-S                   vexasious            vexasious            vexatious\n",
      "   W-IH1-M-ER0                              wimmer               wimmer               wimmer\n",
      "   S-EH1-R-AH0                              sera                 sera                 sarah\n",
      "   HH-EH1-N-IH0-S                           hennis               henis                henness\n",
      "   K-AA1-D-AH0-L                            coddle               coddle               coddle\n",
      "   P-AH0-L-IH1-T-IH0-K-OW2-Z                politicos            politicos            politicos\n",
      "   M-AY1-T-EH2-K                            mightech             mightech             mitek\n",
      "   G-UW0-G-L-IY0-EH0-L-M-EH1-T-IY0          guglielmetti         guglielmetti         guglielmetti\n",
      "   N-IY1-T-OW0                              nito                 nieto                nieto\n",
      "   L-IH1-G-M-AH0-N                          ligman               ligman               ligman\n",
      "   D-EY1-L-AO2-NG                           dailong              dailong              daylong\n",
      "   S-IH1-Z-AH0-L-IH0-NG                     siszling             sizzling             sizzling\n",
      "   Z-OW1-F-AH0-G-AA0-R-T                    zofagart             zofagart             zofagart\n",
      "   IH0-N-S-AY2-K-L-AH0-P-IY1-D-IY0-AH0-Z    encyclopedias        encyclopedias        encyclopedias\n",
      "   K-L-IH1-R-K-AH2-T-IH0-NG                 clearcutting         clearcutting         clearcutting\n",
      "   S-K-AH1-L-P-T-ER0                        sculptor             sculptor             sculptor\n"
     ]
    }
   ],
   "source": [
    "results(preds_beam, preds, test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T10:48:24.391296Z",
     "start_time": "2018-02-08T10:48:24.366088Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(encoder, 'models/encoder07.dat')\n",
    "torch.save(decoder, 'models/decoder07.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T10:48:24.405831Z",
     "start_time": "2018-02-08T10:48:24.392582Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = torch.load('models/encoder07.dat')\n",
    "decoder = torch.load('models/decoder07.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "<ol>\n",
    "<li>[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)</li>\n",
    "<li>[Spro's seq2seq Pytorch Tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)</li> \n",
    "<li>[fast.ai MOOC](http://www.fast.ai/)</li>\n",
    "<li>[AuCson's implementation](https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq/blob/master/attentionRNN.py)</li>\n",
    "<li>[Datalogue Blog about attention](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
