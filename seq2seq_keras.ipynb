{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq - Keras - Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: [CMU Dict](http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b)\n",
    "An introductory Seq2Seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:09.878298Z",
     "start_time": "2018-02-19T07:35:08.859592Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras, keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "Path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:10.095267Z",
     "start_time": "2018-02-19T07:35:09.879607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Code to cap GPU memory usage\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:10.097734Z",
     "start_time": "2018-02-19T07:35:10.096042Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parms = {'verbose': 0, 'callbacks': [TQDMNotebookCallback(leave_inner=True)]}\n",
    "lstm_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:10.453663Z",
     "start_time": "2018-02-19T07:35:10.098548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', ['AH0'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get each word that begins with A-Z from each line into a list \n",
    "lines = [l.strip().split(\"  \") for l in open(Path+'cmudict-0.7b', encoding='latin1') \n",
    "         if re.match('^[A-Z]', l)]\n",
    "#Split words and phonemes\n",
    "lines = [(w, ps.split()) for w, ps in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:11.823024Z",
     "start_time": "2018-02-19T07:35:11.778255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of all the unique phonemes from lines and adding _ to position 0 because it corresponds to padding\n",
    "#when tokenised\n",
    "phonemes = [\"_\"]+sorted(set(p for w, ps in lines for p in ps))\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:12.333871Z",
     "start_time": "2018-02-19T07:35:12.331052Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map phonemes to indices and letters to indices.\n",
    "p2i = dict((v, k) for k, v in enumerate(phonemes))\n",
    "letters = \"_abcdefghijklmnopqrstuvwxyz*\"\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:13.643660Z",
     "start_time": "2018-02-19T07:35:13.423741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 15\n",
    "#Map words to corresponding list of phoneme indices. Constraint\n",
    "pronounce_dict = {w.lower(): [p2i[p] for p in ps] for w, ps in lines\n",
    "                    if (5<=len(w)<=maxlen) and re.match(\"^[A-Z]+$\", w)}\n",
    "len(pronounce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:13.751240Z",
     "start_time": "2018-02-19T07:35:13.726197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_p = max([len(v) for k,v in pronounce_dict.items()]); maxlen_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:37:36.867243Z",
     "start_time": "2018-02-19T07:37:36.856567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hawkins', 'sleeping', 'melchert', ..., 'dedham', 'sleuth', 'herron'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:35:15.244158Z",
     "start_time": "2018-02-19T07:35:15.198908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words contain the number of words in the filtered dictionary\n",
    "words = np.random.permutation(list(pronounce_dict.keys()))\n",
    "n = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T06:56:00.441655Z",
     "start_time": "2018-02-19T06:55:59.966530Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Initialise the input and labels array with zeros so that everywhere except \n",
    "#the position of values is padded\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "#Fill in the non zero indices\n",
    "for i, k in enumerate(words):\n",
    "    for j, p in enumerate(pronounce_dict[k]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:41:55.528848Z",
     "start_time": "2018-02-19T07:41:55.525271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['proglacial', 'caylor', 'culhane', 'bloodstains', 'conversation',\n",
       "       'racal', 'twined', 'dedham', 'sleuth', 'herron'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:12.401693Z",
     "start_time": "2018-02-19T07:45:12.379210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935323"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thefile = open('test2.txt', 'w')\n",
    "thefile.write(\"\\n\".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:35.268794Z",
     "start_time": "2018-02-19T07:45:35.149189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [l.strip().split(\"  \") for l in open(Path+'bpe.txt', encoding='latin1')]\n",
    "\n",
    "clines = [sentence[0].replace(\" \", \"\") for sentence in lines]\n",
    "\n",
    "sep = [sentence.split('@@') for sentence in clines]\n",
    "\n",
    "flat_list = [item for sublist in sep for item in sublist]\n",
    "\n",
    "letters = list(set(flat_list))\n",
    "letters.insert(0,'_')\n",
    "n = len(sep); n\n",
    "\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))\n",
    "\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "for i, k in enumerate(sep):\n",
    "    for j, p in enumerate(pronounce_dict[''.join(k)]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "    \n",
    "\n",
    "labels_.shape\n",
    "\n",
    "[letters[l] for l in labels_[0]]\n",
    "\n",
    "[phonemes[p] for p in input_[0]]\n",
    "\n",
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test) = train_test_split(input_, labels_, test_size=0.1)\n",
    "\n",
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:35.943641Z",
     "start_time": "2018-02-19T07:45:35.891556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clines = [sentence[0].replace(\" \", \"\") for sentence in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:36.455305Z",
     "start_time": "2018-02-19T07:45:36.345026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sep = [sentence.split('@@') for sentence in clines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:36.804302Z",
     "start_time": "2018-02-19T07:45:36.778950Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in sep for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:39.732320Z",
     "start_time": "2018-02-19T07:45:39.715704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = list(set(flat_list))\n",
    "letters.insert(0,'_')\n",
    "n = len(sep); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:41.335688Z",
     "start_time": "2018-02-19T07:45:41.332296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:44.294925Z",
     "start_time": "2018-02-19T07:45:44.291751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:47.932235Z",
     "start_time": "2018-02-19T07:45:47.579550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, k in enumerate(sep):\n",
    "    for j, p in enumerate(pronounce_dict[''.join(k)]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:52.838655Z",
     "start_time": "2018-02-19T07:45:52.830037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108006, 15)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:56.012948Z",
     "start_time": "2018-02-19T07:45:56.002851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ha', 'w', 'kins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[letters[l] for l in labels_[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:45:57.341794Z",
     "start_time": "2018-02-19T07:45:57.330557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HH',\n",
       " 'AO1',\n",
       " 'K',\n",
       " 'IH0',\n",
       " 'N',\n",
       " 'Z',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[phonemes[p] for p in input_[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:02.992409Z",
     "start_time": "2018-02-19T07:46:02.971871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test) = train_test_split(input_, labels_, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:04.116848Z",
     "start_time": "2018-02-19T07:46:04.106360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 881)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:48:34.441355Z",
     "start_time": "2018-02-19T07:48:34.439465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:05.587317Z",
     "start_time": "2018-02-19T07:46:05.582541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:05.796312Z",
     "start_time": "2018-02-19T07:46:05.792824Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn(return_sequences=True):\n",
    "    return LSTM(dim, dropout=0.1, recurrent_dropout=0.1, \n",
    "                implementation=2, return_sequences=return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:07.810049Z",
     "start_time": "2018-02-19T07:46:05.978129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1163: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#Input after embedding becomes #sample*maxlen_p*120\n",
    "inp = Input((maxlen_p,))\n",
    "x = Embedding(input_vocab_size, 120)(inp)\n",
    "\n",
    "#Encoding stage - form a representation of all the information in the word as a whole\n",
    "#Bidirectional creates a separate RNN for a reverse input which is then concat \n",
    "#with original RNN.\n",
    "x = Bidirectional(get_rnn())(x)\n",
    "x = get_rnn(False)(x)\n",
    "\n",
    "#Decoding stage\n",
    "#RepeatVector is used because RNN in Keras expect a list of inputs rather than a \n",
    "#a single hidden state representation. Another issue is that it is also harder for the RNN to keep\n",
    "#track of what the whole word that needs to be decoded. By repeating, each timestep is preented\n",
    "#with the same vector. \n",
    "x = RepeatVector(maxlen)(x)\n",
    "x = get_rnn()(x)\n",
    "x = get_rnn()(x)\n",
    "x = TimeDistributed(Dense(output_vocab_size, activation='softmax'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:08.016748Z",
     "start_time": "2018-02-19T07:46:08.014968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:10.981567Z",
     "start_time": "2018-02-19T07:46:10.965616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 16, 120)           8400      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 16, 1024)          2592768   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 15, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 15, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 881)           451953    \n",
      "=================================================================\n",
      "Total params: 10,399,297\n",
      "Trainable params: 10,399,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:46:13.631746Z",
     "start_time": "2018-02-19T07:46:13.570468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1265: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1129: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(Adam(), 'sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:48:34.434137Z",
     "start_time": "2018-02-19T07:46:15.118371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44a40d782614d1092e0a548cc98d683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a8f16038e0433f854b1c29e7e28d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=97205), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5677c47eba224859bc14eb1e1584db0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=97205), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638f315c739e4c5b91d67bed72b437df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=97205), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(input_train, np.expand_dims(labels_train, -1),\n",
    "                validation_data=[input_test, np.expand_dims(labels_test, -1)],\n",
    "                batch_size=128, **parms, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights('models/seq2seq_rnn.h5')\n",
    "model.load_weights('models/seq2seq_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:49:00.093864Z",
     "start_time": "2018-02-19T07:48:58.517492Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(input_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:49:00.163698Z",
     "start_time": "2018-02-19T07:49:00.094776Z"
    }
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(preds, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:49:00.181770Z",
     "start_time": "2018-02-19T07:49:00.164485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 9.25840199981e-05\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy', np.mean([all(real==p) for real, p in zip(labels_test, predict)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:49:01.608605Z",
     "start_time": "2018-02-19T07:49:01.589401Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualise predictions\n",
    "def evaluate(predict):\n",
    "    print ('Phonemes_________________________________predictions______label')\n",
    "    for index in range(20):\n",
    "        phoneme = '-'.join([phonemes[p] for p in input_test[index]])\n",
    "        prediction = [letters[l] for l in predict[index]]\n",
    "        real = [letters[l] for l in labels_test[index]]\n",
    "        print (phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(14), \n",
    "               ''.join(real).strip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:49:02.694622Z",
     "start_time": "2018-02-19T07:49:02.674753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonemes_________________________________predictions______label\n",
      "R-IH0-V-IY1-L-Z                          rem            reveals\n",
      "M-AH0-G-IY1-AH0-N                        mcd            mcgeean\n",
      "HH-AA0-Y-AA1-R                           wd             hajjar\n",
      "M-AA0-T-AA0-L-AO1-N                      mcch           matalon\n",
      "K-IH1-N-CH-IH0-L-OW0                     conn           kincheloe\n",
      "L-IH1-Z-AH0-B-EH0-TH                     deut           lizabeth\n",
      "T-R-IH1-G                                fr             trygg\n",
      "B-IH1-D-IH0-NG                           bd             bidding\n",
      "T-R-EH1-M-B-AH0-L-IH0-NG                 pt             trembling\n",
      "S-AW2-Y-UW1-M-AH0                        mct            saouma\n",
      "AH0-B-W-EH1-L-AH0                        mct            abuellah\n",
      "HH-AH1-D-AH0-L-S-T-AH0-N                 hoden          huddleston\n",
      "M-EH1-N-S-ER0                            gn             mencer\n",
      "S-AY1-D-SH-OW2-Z                         mcit           sideshows\n",
      "B-L-AH1-D-S-T-R-IY2-M                    hodk           bloodstream\n",
      "F-UW1-T-AA0-N-Z                          shisman        futons\n",
      "M-AE1-K-IH0-N                            mass           mackin\n",
      "B-L-AE1-N-K-IH0-T                        wd             blanchet\n",
      "G-R-AH1-F-L-IY0                          ge             gruffly\n",
      "B-AA2-R-AA1-T-AH0                        bd             bharata\n"
     ]
    }
   ],
   "source": [
    "evaluate(predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T07:20:39.044836Z",
     "start_time": "2018-02-19T07:20:39.042286Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
