{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention seq2seq - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import re\n",
    "import time, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', ['AH0'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get each word that begins with A-Z from each line into a list \n",
    "lines = [l.strip().split(\"  \") for l in open(Path+'cmudict-0.7b', encoding='latin1') \n",
    "         if re.match('^[A-Z]', l)]\n",
    "#Split words and phonemes\n",
    "lines = [(w, ps.split()) for w, ps in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of all the unique phonemes from lines and adding _ to position 0 because it corresponds to padding\n",
    "#when tokenised\n",
    "phonemes = [\"_\"]+sorted(set(p for w, ps in lines for p in ps))\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map phonemes to indices and letters to indices.\n",
    "p2i = dict((v, k) for k, v in enumerate(phonemes))\n",
    "letters = \"_abcdefghijklmnopqrstuvwxyz*\"\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start of sentence token\n",
    "SOS_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 15\n",
    "#Map words to corresponding list of phoneme indices. Constraint\n",
    "pronounce_dict = {w.lower(): [p2i[p] for p in ps] for w, ps in lines\n",
    "                    if (5<=len(w)<=maxlen) and re.match(\"^[A-Z]+$\", w)}\n",
    "len(pronounce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_p = max([len(v) for k,v in pronounce_dict.items()]); maxlen_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words contain the number of words in the filtered dictionary\n",
    "words = np.random.permutation(list(pronounce_dict.keys()))\n",
    "n = len(words)\n",
    "\n",
    "#Initialise the input and labels array with zeros so that everywhere except \n",
    "#the position of values is padded\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "#Fill in the non zero indices\n",
    "for i, k in enumerate(words):\n",
    "    for j, p in enumerate(pronounce_dict[k]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test, \n",
    "    ) = train_test_split(input_, labels_, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=128):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size//2)\n",
    "        self.grubi = nn.GRU(hidden_size//2, hidden_size//2, dropout=0.1, batch_first=True, num_layers=1,\n",
    "                         bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=0.1,\n",
    "                            num_layers=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #print ('encoder inputs input and hidden = ', input.size(), hidden.size())\n",
    "        #print ('encoder embedding', (self.embedding(input)).size())\n",
    "        x, hidden = self.grubi(self.embedding(input), hidden)\n",
    "        h1,h2 = torch.chunk(hidden, 2, 0)\n",
    "        hidden = torch.cat((h1,h2),2)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        #print ('Encoder output-hidden = ', output.size(), hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    # TODO: other inits\n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(2, batch_size, self.hidden_size//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "\n",
    "        # TODO use transpose of embedding\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #print ('decoder inputs input and hidden = ', input.size(), hidden.size())\n",
    "        #emb = self.embedding(input).unsqueeze(1)\n",
    "        #print ('decoder input = ', emb.size())\n",
    "        # NB: Removed relu\n",
    "        res, hidden = self.gru(input, hidden)\n",
    "        #res, hidden = self.gru2(res, hidden)\n",
    "        #print ('decoder output-hidden = ', res.size(), hidden.size())\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        #print ('final output = ', output.size())\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28, 240)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    decoder_input = encoder_hidden.squeeze()\n",
    "    decoder_input = decoder_input.unsqueeze(1)\n",
    "    #print ('MMYYYY',decoder_input.size())\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)          \n",
    "                #, encoder_output, encoder_outputs)\n",
    "        targ = target_variable[:, di]\n",
    "        #print(decoder_output.size(), targ.size(), target_variable.size())\n",
    "        loss += criterion(decoder_output, targ)\n",
    "        #_, indices = torch.max(decoder_output, 1)\n",
    "        #decoder_input = indices\n",
    "        \n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, print_every=1000, plot_every=100, \n",
    "                learning_rate=0.01):\n",
    "    start = time.time()    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "   \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        training_batch = get_batch(input_train, labels_train, 128)\n",
    "\n",
    "        input_variable = Variable(torch.LongTensor((training_batch[0].astype('int64')))).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(training_batch[1].astype('int64'))).cuda()\n",
    "        \n",
    "        loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, \n",
    "                             decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, \n",
    "                                         epoch / n_epochs * 100, print_loss_avg))\n",
    "        \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_vocab_size, dim).cuda()\n",
    "decoder = DecoderRNN(dim, output_vocab_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 12s (- 6m 12s) (500 3%) 0.2088\n",
      "0m 25s (- 6m 0s) (1000 6%) 0.2048\n",
      "0m 38s (- 5m 47s) (1500 10%) 0.2040\n",
      "0m 51s (- 5m 33s) (2000 13%) 0.1953\n",
      "1m 4s (- 5m 20s) (2500 16%) 0.1907\n",
      "1m 16s (- 5m 6s) (3000 20%) 0.1873\n",
      "1m 29s (- 4m 52s) (3500 23%) 0.1853\n",
      "1m 41s (- 4m 39s) (4000 26%) 0.1787\n",
      "1m 54s (- 4m 26s) (4500 30%) 0.1773\n",
      "2m 7s (- 4m 14s) (5000 33%) 0.1734\n",
      "2m 19s (- 4m 1s) (5500 36%) 0.1698\n",
      "2m 32s (- 3m 48s) (6000 40%) 0.1661\n",
      "2m 44s (- 3m 35s) (6500 43%) 0.1641\n",
      "2m 57s (- 3m 22s) (7000 46%) 0.1630\n",
      "3m 9s (- 3m 9s) (7500 50%) 0.1587\n",
      "3m 22s (- 2m 57s) (8000 53%) 0.1539\n",
      "3m 34s (- 2m 44s) (8500 56%) 0.1563\n",
      "3m 47s (- 2m 31s) (9000 60%) 0.1499\n",
      "4m 0s (- 2m 19s) (9500 63%) 0.1454\n",
      "4m 12s (- 2m 6s) (10000 66%) 0.1441\n",
      "4m 25s (- 1m 53s) (10500 70%) 0.1515\n",
      "4m 38s (- 1m 41s) (11000 73%) 0.1399\n",
      "4m 51s (- 1m 28s) (11500 76%) 0.1334\n",
      "5m 3s (- 1m 15s) (12000 80%) 0.1355\n",
      "5m 16s (- 1m 3s) (12500 83%) 0.1337\n",
      "5m 28s (- 0m 50s) (13000 86%) 0.1364\n",
      "5m 41s (- 0m 37s) (13500 90%) 0.1318\n",
      "5m 53s (- 0m 25s) (14000 93%) 0.1313\n",
      "6m 6s (- 0m 12s) (14500 96%) 0.1231\n",
      "6m 18s (- 0m 0s) (15000 100%) 0.1235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4ab1bc358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpJwF0KIOUjCXQgh5qD/DyAG9h0YDTweAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4ab1bceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder, decoder, 15000, print_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_batch(input_test, labels_test)\n",
    "\n",
    "input_variable = Variable(torch.LongTensor((test_batch[0].astype('int64')))).cuda()\n",
    "target_variable = Variable(torch.LongTensor(test_batch[1].astype('int64'))).cuda()\n",
    "\n",
    "batch_size, input_length = input_variable.size()\n",
    "target_length = target_variable.size()[1]\n",
    "encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    \n",
    "encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "#decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size)).cuda()\n",
    "decoder_input = encoder_hidden.squeeze()\n",
    "decoder_input = decoder_input.unsqueeze(1)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoded_words = []\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)          \n",
    "    _, indices = torch.max(decoder_output, 1)\n",
    "    decoded_words.append(indices)\n",
    "    #decoder_input = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = []\n",
    "for x in decoded_words:\n",
    "    combine.append(x.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine = np.array(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 128)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Phonemes_________________________________predictions____________label\n",
      "   Z-IH1-P-K-AE2-R                          zipkar               zipcar\n",
      "   T-UW2-P-EY1                              topey                toupee\n",
      "   G-R-AE1-S-OW0                            grasso               grasso\n",
      "   K-OW0-M-AA1-T-S-UW1                      komotuu              komatsu\n",
      "   B-AH0-K-AA1-S-AH0                        bakasa               bokassa\n",
      "   TH-IH1-K-L-IY0                           thiclly              thickly\n",
      "   W-AO1-K-S                                wahks                wachs\n",
      "   W-UH1-L-S-T-AH0-N                        wollston             woolston\n",
      "   M-IH1-SH-R-AH0                           mishra               mishra\n",
      "   R-EH2-V-AH0-L-UW1-SH-AH0-N-EH2-R-IY0-Z   revolutionaries      revolutionaries\n",
      "   B-IH0-M-OW1-N                            bemoen               bemoan\n",
      "   S-K-R-OW1-L                              scroll               scroll\n",
      "   M-IH1-S-IH0-V                            missive              missive\n",
      "   M-AA0-L-AA1-NG-G-AH0                     malangga             malanga\n",
      "   P-IH2-Z-AE1-Z                            pizazs               pizzazz\n",
      "   F-EH1-L-OW0-SH-IH2-P                     felalsship           fellowship\n",
      "   SH-EH1-F-IY0-L-D                         sheffield            sheffield\n",
      "   N-EY1-T-IH0-V-IH2-Z-AH0-M                nativism             nativism\n",
      "   M-AA1-R-K-AH2-P-S                        markups              markups\n",
      "   M-OW0-N-T-AA0-N-AA1-R-IY0                montanari            montanari\n",
      "   N-AA2-N-S-T-AA1-P-S                      nonstops             nonstops\n",
      "   AO1-T-OW1-S-P-AH0                        autsppa              autospa\n",
      "   T-R-UW1-AH0-N-T                          truent               truant\n",
      "   K-AH0-N-V-IY1-N-Y-AH0-N-T-L-IY0          conveniently         conveniently\n",
      "   F-IH1-L-AH0-T-IH0-D                      pilited              filleted\n",
      "   W-IY1-L-D-IH0-NG                         wherling             wielding\n",
      "   G-R-EY1-N-IY0                            granny               graney\n",
      "   CH-AE1-P-L-AH0-N-Z                       chapaans             chaplains\n",
      "   K-IH1-L-AH0-N                            killan               kilen\n",
      "   G-AH1-L-AH0-T                            gullet               gullatt\n",
      "   HH-AY1-M-IY0-Z                           hymees               jaimes\n",
      "   W-IH1-S-P-IY0                            whippy               wispy\n",
      "   EH0-K-S-P-EH1-R-AH0-M-EH2-N-T-IH0-NG     experimenting        experimenting\n",
      "   G-R-IH0-G-W-AA1-R                        grigua               gregoire\n",
      "   M-AH0-JH-IH1-SH-AH0-N                    magician             magician\n",
      "   AA2-P-TH-AH0-M-AA1-L-AH0-JH-IH0-S-T      optthhologosis       ophthalmologist\n",
      "   K-ER0-AH1-DH-ER0-Z                       corruthers           caruthers\n",
      "   EY1-V-ER0-B-EH2-K                        averbeck             averbeck\n",
      "   B-IH0-K-AO1-F-S-K-IY0                    bicowski             bykowski\n",
      "   AA0-N-D-OW0-L-IY1-N-AH0                  andolina             andolina\n",
      "   D-IH0-S-R-AH1-P-T-S                      disrupts             disrupts\n",
      "   M-IH1-S-T-AH0-F-AY2-D                    mystified            mystified\n",
      "   L-AE1-N-S-AH0-L-AA2-T                    lanceloo             lancelot\n",
      "   D-AW1-N-T-AW1-N                          downtown             downtown\n",
      "   HH-AA1-S-IH0-K                           hasik                hosick\n",
      "   S-K-AY1-S-K-R-EY2-P-ER0                  skyscraper           skyscraper\n",
      "   B-AH0-Z-EH1-L                            bazeelle             buzzell\n",
      "   P-AW1-ER0-IH0-NG                         powering             powering\n",
      "   S-AH1-N-S-T-EY2-T-S                      sunstates            sunstates\n",
      "   F-AO1-L-K-AH0-N-ER0                      falconer             faulconer\n",
      "   N-AY1-S-AH0-N                            nnisen               nicen\n",
      "   S-AA2-IH0-T-AA1-M-AH0                    saiatama             saitama\n",
      "   V-AE1-N-D-ER0                            vander               vander\n",
      "   CH-EH1-K                                 cheche               check\n",
      "   Z-IY1-B-R-AH0                            zibra                zebra\n",
      "   AH0-JH-EY1-S-AH0-N-T                     adaacent             adjacent\n",
      "   S-IH2-TH-ER0-IY1-AH0                     sitheria             cytherea\n",
      "   F-L-AA1-V-IY0-OW0                        flovio               flavio\n",
      "   L-AE1-T-IY0                              latty                latty\n",
      "   T-AE2-T-UW1-D                            tateode              tattooed\n",
      "   B-R-EH1-S-T-F-IY0-D-IH0-NG               breastfidming        breastfeeding\n",
      "   F-EH1-L-T-S                              feltz                felts\n",
      "   T-R-AE0-NG-K-W-IH1-L-IH0-T-IY0           tranquillity         tranquility\n",
      "   IH2-N-D-AH1-S-T-R-IY0-AH0-L-AY2-Z-IH0-NG industrialzini       industrializing\n",
      "   S-AE1-V-IH0-CH                           savich               savitch\n",
      "   M-AO1-R-OW0-Z                            moroz                moros\n",
      "   R-IH0-F-L-EH1-K-T-ER0-Z                  reflecoors           reflectors\n",
      "   K-Y-UW1-T-N-AH0-S                        cutnnss              cuteness\n",
      "   M-AY2-K-R-OW0-AO1-R-G-AH0-N-IH2-Z-AH0-M  microorgagism        microorganism\n",
      "   B-AE1-S-K-AH0-T-S                        baskets              baskets\n",
      "   F-EH1-R-OW0                              farro                pharaoh\n",
      "   B-UW1-M-ER0-AE2-NG-D                     boomrrrune           boomeranged\n",
      "   T-IY1-G-AA2-R-D-AH0-N                    teegarden            teagarden\n",
      "   W-IH1-N-IH0-G-AA0-R-D-N-ER0              winegardnee          winegardner\n",
      "   P-AH2-NG-K-S-AH0-T-AW1-N-IY2             ponssscooae          punxsutawney\n",
      "   S-M-AY1-L-IH0-NG                         smiling              smiling\n",
      "   D-AW1-T-F-AH0-L                          douttful             doubtful\n",
      "   K-IY1-S-ER0                              keeser               kieser\n",
      "   IH0-L-EH1-K-T-R-AH0-F-AY2-Z              electrofies          electrifies\n",
      "   SH-EY1-V-AH0-N                           shaven               shaven\n",
      "   N-AH1-N-L-IY0                            nunley               nunley\n",
      "   S-K-OW1-V-IH2-L                          scoville             scoville\n",
      "   W-AY1-M-AO0-R                            weimore              wymore\n",
      "   EH1-L-V-IY0                              elvie                elvie\n",
      "   SH-AY1-N-B-ER0-G                         sheneberg            scheinberg\n",
      "   K-AA1-K-L-IY0-ER0                        cocllier             cochlear\n",
      "   P-EY1-N-T-ER0                            painter              paynter\n",
      "   R-IY2-P-AH0-Z-EH1-S                      reposessss           repossess\n",
      "   K-R-IH1-S-CH-AH0-N-S-T-EH2-D             crrssshhnnsedd       christiansted\n",
      "   AA0-R-K-EY1-D                            arqaded              arcade\n",
      "   R-IH1-NG                                 winng                ringe\n",
      "   K-R-AH1-M-L-IY0                          crumley              crumly\n",
      "   S-T-R-AE1-N-D                            strand               strand\n",
      "   P-R-EH1-G-N-AH0-N-S-IY0-Z                pregnancies          pregnancies\n",
      "   K-ER1-V-IH0-N                            kurvin               curvin\n",
      "   W-AA1-D-L-IH0-NG-T-AH0-N                 wadlinggon           wadlington\n",
      "   K-AA1-R-L-IY2-N                          carleee              carlene\n",
      "   UW1-Z-B-EH0-K                            uzbeck               uzbek\n",
      "   AW1-T-F-L-AE2-NG-K                       outflank             outflank\n",
      "   L-EH1-K-IY0                              lechye               leckie\n",
      "   K-R-AH1-CH-ER0                           crutcher             crutcher\n",
      "   F-R-AH1-N-T-IH0-JH                       frontige             frontage\n",
      "   M-AA1-B-S-T-ER0                          mobster              mobster\n",
      "   D-AA1-N-Z-B-AA2-K                        donbbock             donsbach\n",
      "   D-IH1-S-T-AH0-L-EY2-T-S                  distilatiss          distillates\n",
      "   IH1-V-AH0-N-AO0-F                        iiannff              ivanoff\n",
      "   K-AH0-M-EH1-N-T-S-AA2-V                  kompenszvv           kamentsev\n",
      "   EH1-L-K-AO2-R                            elccre               elcor\n",
      "   T-ER1-M-IH0-N-AY2                        turminiey            termini\n",
      "   N-IY0-K-OW1-L-IY0                        niccoli              nicoli\n",
      "   IH2-R-IH1-D-IY0-AH0-M                    irridimm             iridium\n",
      "   OW2-V-ER0-R-EH2-G-Y-AH0-L-EY1-SH-AH0-N   overrugulation       overregulation\n",
      "   K-AO1-R-B-IH0-T                          corbett              corbet\n",
      "   R-OW0-N-D-IY0-N-EH1-L-IY0                randinelli           rondinelli\n",
      "   V-AH0-N-EH1-S-AH0                        vanessa              vanessa\n",
      "   EH2-T-S-IY2-OW1-N-IY0                    etiicnn              etzioni\n",
      "   W-ER1-S-AH0-N                            worsen               worsen\n",
      "   N-AH0-K-IY1-AH0                          nakeah               nekia\n",
      "   B-UW0-K-OW1-L-AH0                        buckola              buccola\n",
      "   IH0-M-B-AY1-B-D                          embiied              imbibed\n",
      "   M-AH1-SH-R-UH2-M-IH0-NG                  mushroaming          mushrooming\n",
      "   S-T-R-EH1-V-IH0-G                        strevig              strevig\n",
      "   B-AY0-AH0-L-AA1-JH-IH0-K-S               biologics            biologics\n",
      "   S-K-AY1-L-AY2-T-S                        skylights            skylites\n",
      "   V-AH0-S-IH1-F-ER0-EY0-T                  vociferate           vociferate\n",
      "   AA1-R-T-M-AH0-N                          artman               artman\n",
      "   AH2-N-B-AH0-L-IY1-V-IH0-NG               unbalieving          unbelieving\n",
      "   R-AH1-S-AH0-L-V-IH2-L                    russeeville          russellville\n"
     ]
    }
   ],
   "source": [
    "input_test1=test_batch[0]\n",
    "labels_test1=test_batch[1]\n",
    "print ('  Phonemes_________________________________predictions____________label')\n",
    "for index in range(128):\n",
    "    phoneme = '-'.join([phonemes[p] for p in input_test1[index]])\n",
    "    prediction = [letters[l] for l in combine[:, index]]\n",
    "    real = [letters[l] for l in labels_test1[index]]\n",
    "    print ('  ',phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(20), \n",
    "           ''.join(real).strip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder, 'models/encoder.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(decoder, 'models/decoder.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
