{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention seq2seq - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b <br>\n",
    "The goal of this notebook is to implement a seq2seq attention model and a regular seq2seq is implemented alongside to gain a more complete picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this notebook contains mapping of words in English and their pronounciations as a set of phoneme word pairs like so <br>\n",
    "<small><p style=\"margin-left: 40px\">S-AE1-N-AH0-T-IY0   sanity</p></small>\n",
    "The task is to use a seq2seq model to learn this mapping so that given a set of phonemes, the model outputs the correct word. It can be seen as emulating spelling bee. Given below is an image of the results obtained from a regular seq2seq model\n",
    "<img src=\"seq2seq.png\">\n",
    "The words inside the red square shows a common problem with seq2seq- If only the context vector, i.e., the last hidden state in the encoder network is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence, extending upto the last timestep of the decoder. \n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every time step of the decoder's own outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.159604Z",
     "start_time": "2018-02-05T16:24:32.500257Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "import time, math, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "Path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.164594Z",
     "start_time": "2018-02-05T16:24:33.160662Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.529738Z",
     "start_time": "2018-02-05T16:24:33.165596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', ['AH0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get each word that begins with A-Z from each line into a list \n",
    "lines = [l.strip().split(\"  \") for l in open(Path+'cmudict-0.7b', encoding='latin1') \n",
    "         if re.match('^[A-Z]', l)]\n",
    "#Split words and phonemes\n",
    "lines = [(w, ps.split()) for w, ps in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.589621Z",
     "start_time": "2018-02-05T16:24:33.530742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of all the unique phonemes from lines and adding _ to position 0 because it corresponds to padding\n",
    "#when tokenised\n",
    "phonemes = [\"_\"]+sorted(set(p for w, ps in lines for p in ps))\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.593201Z",
     "start_time": "2018-02-05T16:24:33.590612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map phonemes to indices and letters to indices.\n",
    "p2i = dict((v, k) for k, v in enumerate(phonemes))\n",
    "letters = \"_abcdefghijklmnopqrstuvwxyz*\"\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.596930Z",
     "start_time": "2018-02-05T16:24:33.594123Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start of sentence token\n",
    "SOS_token = 0\n",
    "\n",
    "maxlen = 15\n",
    "#Map words to corresponding list of phoneme indices. Constraint\n",
    "pronounce_dict = {w.lower(): [p2i[p] for p in ps] for w, ps in lines\n",
    "                    if (5<=len(w)<=maxlen) and re.match(\"^[A-Z]+$\", w)}\n",
    "len(pronounce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:33.845544Z",
     "start_time": "2018-02-05T16:24:33.834794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_p = max([len(v) for k,v in pronounce_dict.items()]); maxlen_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.389077Z",
     "start_time": "2018-02-05T16:24:33.846405Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words contain the number of words in the filtered dictionary\n",
    "words = np.random.permutation(list(pronounce_dict.keys()))\n",
    "n = len(words)\n",
    "\n",
    "#Initialise the input and labels array with zeros so that everywhere except \n",
    "#the position of values is padded\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "#Fill in the non zero indices\n",
    "for i, k in enumerate(words):\n",
    "    for j, p in enumerate(pronounce_dict[k]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.399165Z",
     "start_time": "2018-02-05T16:24:34.390018Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test, \n",
    "    ) = train_test_split(input_, labels_, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.402772Z",
     "start_time": "2018-02-05T16:24:34.400134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.411918Z",
     "start_time": "2018-02-05T16:24:34.407509Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=128):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.406559Z",
     "start_time": "2018-02-05T16:24:34.404258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 240\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.436400Z",
     "start_time": "2018-02-05T16:24:34.412844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size//2)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.grubi = nn.GRU(hidden_size//2, hidden_size//2, dropout=dropout_p, batch_first=True, num_layers=1,\n",
    "                         bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=dropout_p,\n",
    "                            num_layers=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x, hidden = self.grubi(x, hidden)\n",
    "        #Concatenating hidden state to get a single layer because\n",
    "        #bidirectional return a layer for each direction. \n",
    "        hidden = torch.cat(torch.chunk(hidden, 2, 0),2)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #2 for bidirectional, change to 1 otherwise.\n",
    "        return Variable(torch.zeros(2, batch_size, self.hidden_size//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.457566Z",
     "start_time": "2018-02-05T16:24:34.437401Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) #Optional\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #emb = input\n",
    "        #Comment above line and uncomment below to change decoder inputs as being\n",
    "        #target values/outputs\n",
    "        #print ('Decoded Input', input.size())\n",
    "        emb = self.embedding(input).unsqueeze(1)\n",
    "        #print ('Decoder embedded Input', emb.size())\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        res, hidden = self.gru2(res, hidden)\n",
    "        #print ('decoder output - hidden', res.size(), hidden.size())\n",
    "        #print ('meaning of res[:,0]', res[:,0].size())\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "#### Attention module\n",
    "<img src=\"formula.png\">\n",
    "Attn class implements the above equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.476200Z",
     "start_time": "2018-02-05T16:24:34.458583Z"
    },
    "cell_style": "center",
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(nn.init.xavier_normal(torch.rand(1, hidden_size)))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        Inputs:\n",
    "        hidden - hidden state of previous layer \n",
    "        encoder_outputs - encoder outputs for each timestep [BxTxH]\n",
    "        Output: \n",
    "        Normalized weightings for each of the encoder output [BxT]\n",
    "        '''\n",
    "        maxlen_p = encoder_outputs.size(1) \n",
    "        H = hidden.repeat(maxlen_p, 1, 1).transpose(0,1)\n",
    "        weights = self.attn(torch.cat([H, encoder_outputs], 2)) #[BxTx2H]->[BxTxH]\n",
    "        weights = self.tanh(weights) \n",
    "        weights = weights.transpose(2,1) #[BxHxT]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[Bx1xH]\n",
    "        weights = torch.bmm(v, weights) #[Bx1xH]*[BxHxT]->[Bx1xT]\n",
    "        weights = weights.squeeze(1) #softmax required 2D tensor\n",
    "        return self.softmax(weights).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:24:34.502186Z",
     "start_time": "2018-02-05T16:24:34.477192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, maxlen_p, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.maxlen_p = maxlen_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, batch_first=True, num_layers=1, dropout=dropout_p)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden, enc_outputs):\n",
    "        emb = self.embedding(input).unsqueeze(1)\n",
    "        emb = self.dropout(emb)\n",
    "        weights = self.attn(hidden, enc_outputs)        \n",
    "        context = weights.bmm(enc_outputs) \n",
    "        rnn_input = torch.cat((emb, context), 2)\n",
    "        \n",
    "        res, hidden = self.gru(rnn_input, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T18:56:52.241827Z",
     "start_time": "2018-02-05T18:56:52.188460Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "def train(attention, input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, teach_force_p=0):\n",
    "    batch_size, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size)).cuda()\n",
    "    \n",
    "    if (random.random() > teach_force_p):\n",
    "        for di in range(target_length):\n",
    "            if attention:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)          \n",
    "\n",
    "            targ = target_variable[:, di]\n",
    "            loss += criterion(decoder_output, targ)\n",
    "            decoder_input = targ\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            if attention:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)                    \n",
    "            \n",
    "            targ = target_variable[:, di]\n",
    "            loss += criterion(decoder_output, targ)\n",
    "            _, indices = torch.max(decoder_output, 1)\n",
    "            decoder_input = indices\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T18:56:52.651552Z",
     "start_time": "2018-02-05T18:56:52.616466Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(attention, encoder, decoder, iters, print_every=1000, plot_every=100, \n",
    "                learning_rate=0.01):\n",
    "\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "\n",
    "    for iter in tqdm_notebook(range(1, iters + 1)):\n",
    "        training_batch = get_batch(input_train, labels_train, 128)\n",
    "        input_variable = Variable(torch.LongTensor((training_batch[0].astype('int64')))).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(training_batch[1].astype('int64'))).cuda()\n",
    "        #iter/iters is passed for teacher forcing probability to reduce it in the later iters. \n",
    "        loss = train(attention, input_variable, target_variable, encoder, decoder, encoder_optimizer, \n",
    "                             decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Loss: ',print_loss_avg, end=\"\\r\", flush=True)\n",
    "            \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T18:56:53.542194Z",
     "start_time": "2018-02-05T18:56:53.537837Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T18:56:54.033299Z",
     "start_time": "2018-02-05T18:56:53.960882Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_vocab_size, dim).cuda()\n",
    "#decoder = DecoderRNN(dim, output_vocab_size).cuda()\n",
    "attndecoder = AttnDecoderRNN(dim, output_vocab_size, maxlen_p).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T19:03:05.690283Z",
     "start_time": "2018-02-05T18:56:54.752512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608e23bddf6f4bf4809d2e7c55862da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.13558609283765152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2f76b45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1y3QUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2f7b54eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(True, encoder, attndecoder, 5000, print_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T19:03:58.269014Z",
     "start_time": "2018-02-05T19:03:58.227627Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(attention, encoder, decoder):  \n",
    "    test_batch = get_batch(input_test, labels_test, batch_size*8)\n",
    "\n",
    "    input_variable = Variable(torch.LongTensor((test_batch[0].astype('int64'))), volatile=True).cuda()\n",
    "    target_variable = Variable(torch.LongTensor(test_batch[1].astype('int64')), volatile=True).cuda()\n",
    "\n",
    "    _, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size*8).cuda()\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size*8), volatile=True).cuda()\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    for di in range(target_length):\n",
    "        if attention:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)                \n",
    "        _, indices = torch.max(decoder_output, 1)\n",
    "        decoded_words.append(indices)\n",
    "        decoder_input = indices\n",
    "    preds = []\n",
    "    for x in decoded_words:\n",
    "        preds.append(x.cpu().data.numpy())\n",
    "    preds = np.array(preds).T\n",
    "    print ('Accuracy', np.mean([all(real==p) for real, p in zip(test_batch[1], preds)])*100,'%')\n",
    "    \n",
    "    print ('  Phonemes_________________________________predictions____________label')\n",
    "    for index in range(32):\n",
    "        phoneme = '-'.join([phonemes[p] for p in test_batch[0][index]])\n",
    "        prediction = [letters[l] for l in preds[index]]\n",
    "        real = [letters[l] for l in test_batch[1][index]]\n",
    "        print ('  ',phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(20), \n",
    "               ''.join(real).strip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T19:04:11.474885Z",
     "start_time": "2018-02-05T19:04:11.415672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 46.38671875 %\n",
      "  Phonemes_________________________________predictions____________label\n",
      "   R-AA1-K-ER0-Z                            rockers              rockers\n",
      "   M-EY1-D-AH0-L                            maidle               maidel\n",
      "   HH-AE1-M-ER0-S-L-IY0                     hammersly            hammersley\n",
      "   S-AA0-M-AO1-R-AH0                        samora               samora\n",
      "   B-IH0-D-EH1-V-AH0-L                      bedevile             bedevil\n",
      "   AH0-G-R-IY1-V-D                          aggrieved            aggrieved\n",
      "   K-AE1-R-AH0-T                            carrott              carrot\n",
      "   S-N-AE1-P-D-R-AE2-G-AH0-N                snapdragen           snapdragon\n",
      "   K-AA1-M-AH0-S-T                          commust              calmest\n",
      "   W-AO1-S-AH0-L                            wasle                wassell\n",
      "   CH-AY1-K-AH0                             chika                czajka\n",
      "   AA2-K-W-AH0-M-ER0-IY1-N                  acqumarine           aquamarine\n",
      "   T-AH1-S-AH0-L                            tussel               tussle\n",
      "   S-T-OW1-IH0-K                            stoick               stoic\n",
      "   V-IH1-T-R-IH0-K                          vitrick              vitric\n",
      "   L-IY1-F-IY0                              liefee               leafy\n",
      "   N-UW1-T-S-AH0-N                          nutson               knutson\n",
      "   S-P-EH1-N-ER0                            spenner              spenner\n",
      "   P-AE1-T-AH0-N-Z                          patons               patons\n",
      "   G-R-UW1-V-ER0                            grover               groover\n",
      "   D-AH0-B-OW1                              dubeau               dubeau\n",
      "   K-AE1-R-IH0-L-AA0-K                      carillock            carelock\n",
      "   B-AH0-L-IY1-M-IY0-AH0                    belimia              bulemia\n",
      "   M-AA0-N-T-IY2-CH-IY0-OW1-L-OW0           monticiolo           monticciolo\n",
      "   K-R-IY0-EY1-T-S                          creates              creates\n",
      "   S-K-AA1-L-ER0                            scollar              scholar\n",
      "   S-AH1-L-F-EY2-T                          sulfate              sulfate\n",
      "   HH-AE1-M-B-ER0-G                         hamberg              hamburg\n",
      "   G-AA1-D-F-R-IY0                          godfrey              godfrey\n",
      "   SH-IY0-Z-W-OW1-K-AH0                     shizwoka             shizuoka\n",
      "   R-AE1-D-M-AH0-N-D                        radmund              radmund\n",
      "   K-AA1-T-AH0-N                            kotton               cotton\n"
     ]
    }
   ],
   "source": [
    "evaluate(True, encoder, attndecoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T17:32:46.698933Z",
     "start_time": "2018-02-05T17:32:46.678550Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(encoder, 'models/encoder.dat')\n",
    "torch.save(decoder, 'models/decoder.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder = torch.load('models/encoder.dat')\n",
    "decoder = torch.load('models/decoder.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "<ol>\n",
    "<li>[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)</li>\n",
    "<li>[Spro's seq2seq Pytorch Tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)</li> \n",
    "<li>[fast.ai MOOC](http://www.fast.ai/)</li>\n",
    "<li>[AuCson's implementation](https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq/blob/master/attentionRNN.py)</li>\n",
    "</ol>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
