{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention seq2seq - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b <br>\n",
    "The goal of this notebook is to implement a seq2seq attention model and a regular seq2seq is implemented alongside to gain a more complete picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this notebook contains mapping of words in English and their pronounciations as a set of phoneme word pairs like so <br>\n",
    "<small><p style=\"margin-left: 40px\">S-AE1-N-AH0-T-IY0   sanity</p></small>\n",
    "The task is to use a seq2seq model to learn this mapping so that given a set of phonemes, the model outputs the correct word. It can be seen as emulating spelling bee. Given below is an image of the results obtained from a regular seq2seq model\n",
    "<img src=\"seq2seq.png\">\n",
    "The words inside the red square shows a common problem with seq2seq- If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence. \n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "import time, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "Path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', ['AH0'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get each word that begins with A-Z from each line into a list \n",
    "lines = [l.strip().split(\"  \") for l in open(Path+'cmudict-0.7b', encoding='latin1') \n",
    "         if re.match('^[A-Z]', l)]\n",
    "#Split words and phonemes\n",
    "lines = [(w, ps.split()) for w, ps in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a list of all the unique phonemes from lines and adding _ to position 0 because it corresponds to padding\n",
    "#when tokenised\n",
    "phonemes = [\"_\"]+sorted(set(p for w, ps in lines for p in ps))\n",
    "len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map phonemes to indices and letters to indices.\n",
    "p2i = dict((v, k) for k, v in enumerate(phonemes))\n",
    "letters = \"_abcdefghijklmnopqrstuvwxyz*\"\n",
    "l2i = dict((v, k) for k, v in enumerate(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start of sentence token\n",
    "SOS_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 15\n",
    "#Map words to corresponding list of phoneme indices. Constraint\n",
    "pronounce_dict = {w.lower(): [p2i[p] for p in ps] for w, ps in lines\n",
    "                    if (5<=len(w)<=maxlen) and re.match(\"^[A-Z]+$\", w)}\n",
    "len(pronounce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_p = max([len(v) for k,v in pronounce_dict.items()]); maxlen_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words contain the number of words in the filtered dictionary\n",
    "words = np.random.permutation(list(pronounce_dict.keys()))\n",
    "n = len(words)\n",
    "\n",
    "#Initialise the input and labels array with zeros so that everywhere except \n",
    "#the position of values is padded\n",
    "input_ = np.zeros((n, maxlen_p), np.int32)\n",
    "labels_ = np.zeros((n, maxlen), np.int32)\n",
    "\n",
    "#Fill in the non zero indices\n",
    "for i, k in enumerate(words):\n",
    "    for j, p in enumerate(pronounce_dict[k]): input_[i][j]=p\n",
    "    for j, p in enumerate(k): labels_[i][j] = l2i[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train, validation sets\n",
    "(input_train, input_test, labels_train, labels_test, \n",
    "    ) = train_test_split(input_, labels_, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, output_vocab_size = len(phonemes), len(letters);input_vocab_size, output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 240\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=128):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size//2)\n",
    "        self.grubi = nn.GRU(hidden_size//2, hidden_size//2, dropout=0.1, batch_first=True, num_layers=1,\n",
    "                         bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=0.1,\n",
    "                            num_layers=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #print ('encoder inputs input and hidden = ', input.size(), hidden.size())\n",
    "        #print ('encoder embedding', (self.embedding(input)).size())\n",
    "        x, hidden = self.grubi(self.embedding(input), hidden)\n",
    "        #Concatenating hidden state to get a single layer because\n",
    "        #bidirectional return a layer for each direction. \n",
    "        hidden = torch.cat(torch.chunk(hidden, 2, 0),2)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        #print ('Encoder output-hidden = ', output.size(), hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    # TODO: other inits\n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(2, batch_size, self.hidden_size//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) #Optional\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #emb = input\n",
    "        #Comment above line and uncomment below to change decoder inputs as being\n",
    "        #target values/outputs\n",
    "        #print ('Decoded Input', input.size())\n",
    "        emb = self.embedding(input).unsqueeze(1)\n",
    "        #print ('Decoder embedded Input', emb.size())\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        res, hidden = self.gru2(res, hidden)\n",
    "        #print ('decoder output - hidden', res.size(), hidden.size())\n",
    "        #print ('meaning of res[:,0]', res[:,0].size())\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        print ('decoder ouput shape', output.size())\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "        # end of update\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "        attn_energies = self.score(H,encoder_outputs) # compute attention score\n",
    "        return self.softmax(attn_energies).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        #print ('hidden and encoder', hidden.size(), encoder_outputs.size())\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2)) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attnn(nn.Module):\n",
    "    def __init__(self, hidden_size, max_length):\n",
    "        super(Attn, self).__init__()\n",
    "        self.maxlen = max_length\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.other = nn.Parameter(weight_init.xavier_uniform(torch.FloatTensor(1, self.hidden_size)))\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Create variable to store attention weights\n",
    "        weights = Variable(torch.zeros(self.maxlen)).cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(self.maxlen):\n",
    "            attn_weights[i] = self.score(hidden, encoder_outputs[i])\n",
    "        return F.softmax(attn_weights).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        weight = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "        print ('weight and other', weight.size(), self.other.size())\n",
    "        weight = torch.matmul(weight,other.transpose(0,1))\n",
    "        print(weight.size())\n",
    "        return weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, maxlen_p, n_layers=1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.maxlen_p = maxlen_p\n",
    "        self.fix = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size) \n",
    "        self.attn = Attn(hidden_size)\n",
    "        #self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden, enc_outputs):\n",
    "        emb = self.embedding(input).unsqueeze(1)\n",
    "        #Variable for storing attention weights\n",
    "        attn_weights = Variable(torch.zeros(self.maxlen_p)).cuda()\n",
    "        #print ('encode outputs', enc_outputs.size())\n",
    "        alphas = self.attn(hidden, enc_outputs)        \n",
    "        #print ('alphas ', alphas.size())\n",
    "        #print ('alphas are going to be bmmed with enc outputs transpose', \n",
    "        #enc_outputs.transpose(0,1).size())\n",
    "        context = alphas.bmm(enc_outputs) # B x 1 x N\n",
    "        #print ('emb and context going to be catted', emb.size(), context.size())\n",
    "        rnn_input = torch.cat((emb, context), 2)\n",
    "        rnn_input = self.fix(rnn_input)\n",
    "        #print ('input to gru', rnn_input.size())\n",
    "        res, hidden = self.gru(rnn_input, hidden)\n",
    "        res, hidden = self.gru2(res, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 128])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other = torch.FloatTensor(1, 128, 240); other[-1].transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainattn(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size)).cuda()\n",
    "    #Below code uses context vector as input to all timesteps of decoder. Comment above line\n",
    "    #and uncomment below to change to this. \n",
    "    #decoder_input = encoder_hidden.squeeze()\n",
    "    #decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)          \n",
    "        targ = target_variable[:, di]\n",
    "        loss += criterion(decoder_output, targ)\n",
    "        #Uncomment below to do teacher forcing\n",
    "        decoder_input = targ\n",
    "        #Comment above line and uncomment below to take model output as input \n",
    "        #_, indices = torch.max(decoder_output, 1)\n",
    "        #decoder_input = indices\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size)).cuda()\n",
    "    #Below code uses context vector as input to all timesteps of decoder. Comment above line\n",
    "    #and uncomment below to change to this. \n",
    "    #decoder_input = encoder_hidden.squeeze()\n",
    "    #decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)          \n",
    "        targ = target_variable[:, di]\n",
    "        loss += criterion(decoder_output, targ)\n",
    "        #Uncomment below to do teacher forcing\n",
    "        decoder_input = targ\n",
    "        #Comment above line and uncomment below to take model output as input \n",
    "        #_, indices = torch.max(decoder_output, 1)\n",
    "        #decoder_input = indices\n",
    "        break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, print_every=1000, plot_every=100, \n",
    "                learning_rate=0.01):\n",
    "\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "   \n",
    "    for epoch in tqdm_notebook(range(1, n_epochs + 1)):\n",
    "        training_batch = get_batch(input_train, labels_train, 128)\n",
    "        input_variable = Variable(torch.LongTensor((training_batch[0].astype('int64')))).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(training_batch[1].astype('int64'))).cuda()\n",
    "        \n",
    "        loss = trainattn(input_variable, target_variable, encoder, decoder, encoder_optimizer, \n",
    "                             decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Loss: ',print_loss_avg, end=\"\\r\", flush=True)        \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_vocab_size, dim).cuda()\n",
    "decoder = DecoderRNN(dim, output_vocab_size).cuda()\n",
    "attndecoder = AttnDecoderRNN(dim, output_vocab_size, maxlen_p).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e974c2b328a84ec3a1161f9184b7f007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.15077918004989612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f50d78512b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7iIiDqRyFxFxIJW7iIgDqdxFRBzo/wBtlGtWapI7yAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f50f1cc9470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder, attndecoder, 3500, print_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.rand(3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.9820  0.9743  0.5090  0.9338\n",
       "  0.7524  0.5077  0.0212  0.2963\n",
       "  0.3436  0.3038  0.5135  0.6495\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.7633  0.5293  0.5958  0.2039\n",
       "  0.4689  0.9291  0.6631  0.4143\n",
       "  0.7419  0.8946  0.6434  0.7140\n",
       "[torch.FloatTensor of size 2x3x4]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(a,0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f2fa2180284e4fb950ce049873aa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder inputs input and hidden =  torch.Size([128, 16]) torch.Size([2, 128, 120])\n",
      "encoder embedding torch.Size([128, 16, 120])\n",
      "Encoder output-hidden =  torch.Size([128, 16, 240]) torch.Size([1, 128, 240])\n",
      "Decoded Input torch.Size([128])\n",
      "Decoder embedded Input torch.Size([128, 1, 240])\n",
      "decoder output - hidden torch.Size([128, 1, 240]) torch.Size([1, 128, 240])\n",
      "meaning of res[:,0] torch.Size([128, 240])\n",
      "decoder ouput shape torch.Size([128, 28])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f510a531e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDf0HxR+aRG5JPtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f510a8f5ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder, decoder, 1, print_every=500, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder):  \n",
    "    test_batch = get_batch(input_test, labels_test, batch_size*8)\n",
    "\n",
    "    input_variable = Variable(torch.LongTensor((test_batch[0].astype('int64'))), volatile=True).cuda()\n",
    "    target_variable = Variable(torch.LongTensor(test_batch[1].astype('int64')), volatile=True).cuda()\n",
    "\n",
    "    _, input_length = input_variable.size()\n",
    "    target_length = target_variable.size()[1]\n",
    "    encoder_hidden = encoder.initHidden(batch_size*8).cuda()\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]*batch_size*8), volatile=True).cuda()\n",
    "    #decoder_input = encoder_hidden.squeeze()\n",
    "    #decoder_input = decoder_input.unsqueeze(1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)          \n",
    "        _, indices = torch.max(decoder_output, 1)\n",
    "        decoded_words.append(indices)\n",
    "        decoder_input = indices\n",
    "    combine = []\n",
    "    for x in decoded_words:\n",
    "        combine.append(x.cpu().data.numpy())\n",
    "    combine = np.array(combine).T\n",
    "    print ('Accuracy', np.mean([all(real==p) for real, p in zip(test_batch[1], combine)])*100,'%')\n",
    "    return test_batch, combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 41.89453125 %\n"
     ]
    }
   ],
   "source": [
    "test_batch, preds = evaluate(encoder, attndecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Phonemes_________________________________predictions____________label\n",
      "   P-R-OW1-T-OW0-T-AY2-P-IH0-NG             prototyping          prototyping\n",
      "   M-EY1-JH-IH0-Z                           mages                mages\n",
      "   SH-ER1-Z                                 schers               scherz\n",
      "   R-OW1-L-K                                rolke                roelke\n",
      "   D-AW1-D-AH0-L                            dowdell              dowdell\n",
      "   HH-AE1-L-K-AH0-M                         halcomb              halcomb\n",
      "   Y-ER0-AA1-L-AH0-JH-AH0-S-T-S             yerologists          urologists\n",
      "   S-AY1-D-AA0-R-M                          siderm               sidearm\n",
      "   T-AE1-SH                                 tasch                tasch\n",
      "   AE1-Z-AY2-D-Z                            asides               azides\n",
      "   R-AA1-K-L-AO2                            rocklaugh            wroclaw\n",
      "   S-W-IH1-N-D-AH0-L-HH-ER0-S-T             swindlearchers       swindlehurst\n",
      "   D-AE1-Z-OW0                              dazzo                dazzo\n",
      "   R-IH1-R-D-AH0-N                          rerden               reardon\n",
      "   M-AO1-S-ER0                              mauser               mosser\n",
      "   N-EY1-T-AH0-L                            natel                natal\n",
      "   S-AH0-S-P-EH1-K-T                        suspect              suspect\n",
      "   L-OW2-K-AH0-M-OW1-T-IH0-V                locomottive          locomotive\n",
      "   B-EH1-NG-K-ER0                           benker               benker\n",
      "   F-L-EH1-R                                flair                flair\n",
      "   S-K-UW1-T                                scoot                scoot\n",
      "   F-AH0-L-AE1-N-TH-R-AH0-P-AH0-S-T         falinthrops          philanthropist\n",
      "   SH-L-UH1-T-ER0                           schluter             schlueter\n",
      "   F-IH1-D-AH0-L-Z                          fiddles              fiddles\n",
      "   R-EH1-S-IH0-N-D-Z                        rescinds             resendes\n",
      "   EH1-S-K-AH0-L-EY2-T-S                    escolates            escalates\n",
      "   S-AH1-L-IH0-V-AH0-N-Z                    sullivans            sullivans\n",
      "   M-AH0-K-D-UW1-G-AH0-L-Z                  mcdougals            mcdougals\n",
      "   P-OW1-L-EH0-K                            polek                polek\n",
      "   D-W-EH1-L-IY0                            dwelley              dwelley\n",
      "   K-R-IY0-EY1-T-IH0-V-L-IY0                creatively           creatively\n",
      "   JH-AO1-N-B-AH0-N-EY1                     jonbane              jonbenet\n"
     ]
    }
   ],
   "source": [
    "input_test=test_batch[0]\n",
    "labels_test=test_batch[1]\n",
    "print ('  Phonemes_________________________________predictions____________label')\n",
    "for index in range(32):\n",
    "    phoneme = '-'.join([phonemes[p] for p in test_batch[0][index]])\n",
    "    prediction = [letters[l] for l in preds[index]]\n",
    "    real = [letters[l] for l in test_batch[1][index]]\n",
    "    print ('  ',phoneme.strip('-_').ljust(40), ''.join(prediction).strip('_').ljust(20), \n",
    "           ''.join(real).strip('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, 'models/encoder.dat')\n",
    "torch.save(decoder, 'models/decoder.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('models/encoder.dat')\n",
    "decoder = torch.load('models/decoder.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
